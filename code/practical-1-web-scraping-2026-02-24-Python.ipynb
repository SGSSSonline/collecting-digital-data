{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SGSSS Logo](../img/SGSSS_Stacked.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Digital Data for Social Scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Computational methods are transforming the social sciences, enabling researchers to collect, analyse, and interpret data at scales and speeds that were previously impossible. One of the most powerful techniques in this toolkit is **web scraping** — the automated extraction of information from websites. Web scraping allows social scientists to create new datasets from digital sources, turning the vast and often unstructured content of the internet into structured, analysable data.\n",
    "\n",
    "This practical session introduces you to web scraping using Python. We will start with a simple example — extracting text from a single web page — and then move on to a more realistic scenario involving multiple pages. By the end of this session, you will have a solid foundation for collecting digital data from the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aims\n",
    "\n",
    "1. **Demonstrate how Python can be used for web scraping** — from requesting web pages, to parsing HTML, extracting information, and saving results.\n",
    "2. **Cultivate computational thinking skills** — breaking down a data collection problem into a series of logical, repeatable steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Details\n",
    "\n",
    "| | |\n",
    "| --- | --- |\n",
    "| **Level** | Introductory |\n",
    "| **Time** | ~45 minutes |\n",
    "| **Pre-requisites** | None |\n",
    "| **Learning outcomes** | Understand the key steps involved in web scraping |\n",
    "| | Be able to use Python to request a web page |\n",
    "| | Be able to use Python to parse HTML content |\n",
    "| | Be able to use Python to extract specific information from a web page |\n",
    "| | Be able to use Python to save scraped data to a file |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide to Using This Resource\n",
    "\n",
    "This is a **Jupyter Notebook** — an interactive document that combines text, code, and output in a single environment. If you are viewing this in **Google Colab**, you are running the notebook in the cloud, which means you do not need to install anything on your own machine.\n",
    "\n",
    "A notebook is made up of **cells**. There are two main types:\n",
    "\n",
    "- **Markdown cells** contain formatted text (like this one). They provide explanations, instructions, and context.\n",
    "- **Code cells** contain Python code that you can execute. Code cells are displayed with a grey background and have a play button on the left.\n",
    "\n",
    "To **run a cell**, click on it and press `Shift+Enter` (or click the play button). The output will appear directly below the cell. You should run the code cells **in order**, from top to bottom, as later cells often depend on variables or modules defined in earlier cells.\n",
    "\n",
    "If you are new to Jupyter Notebooks and would like a more detailed introduction, see the excellent materials by Dani Arribas-Bel: [https://github.com/darribas/gds19/blob/master/content/labs/lab_00.ipynb](https://github.com/darribas/gds19/blob/master/content/labs/lab_00.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Enter your name and press enter:\")\n",
    "name = input()\n",
    "print(f\"\\nHello {name}, enjoy learning about Python and web scraping!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Approach\n",
    "\n",
    "Web scraping follows a consistent pattern regardless of the website or the data you want to collect. Before writing any code, there are things you need to **KNOW** and things you need to **DO**.\n",
    "\n",
    "**What you need to KNOW:**\n",
    "- The **URL** (web address) of the page(s) containing the data you want.\n",
    "- The **HTML structure** of the page — specifically, which HTML tags and attributes contain the information you need.\n",
    "\n",
    "**What you need to DO:**\n",
    "1. **Request** the web page (download the HTML).\n",
    "2. **Parse** the HTML (turn the raw text into a structured, searchable object).\n",
    "3. **Extract** the specific information you need.\n",
    "4. **Save** the results to a file.\n",
    "\n",
    "This four-step pattern — request, parse, extract, save — is the foundation of nearly all web scraping tasks. It can be expressed as **pseudo-code**, which is an informal, plain-language description of the steps a program needs to follow. Writing pseudo-code before you write real code is an excellent habit: it helps you think through the logic of your task without getting bogged down in syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Text Extraction\n",
    "\n",
    "We will begin with a simple example: extracting a passage of text from a single web page. The website we will use is [httpbin.org/html](https://httpbin.org/html), which serves a short excerpt from *Moby Dick* by Herman Melville. This is a deliberately simple page, which makes it ideal for learning the basics of web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the web address\n",
    "\n",
    "The first thing we need to know is the **URL** of the page we want to scrape. In this case, the address is:\n",
    "\n",
    "> [https://httpbin.org/html](https://httpbin.org/html)\n",
    "\n",
    "If you visit this URL in your browser, you will see a short passage of text from *Moby Dick*. This is the data we want to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locating information in the HTML\n",
    "\n",
    "Web pages are written in **HTML** (HyperText Markup Language). HTML uses **tags** to structure content. For example, a paragraph of text is enclosed in `<p>` tags:\n",
    "\n",
    "```html\n",
    "<p>This is a paragraph.</p>\n",
    "```\n",
    "\n",
    "To scrape a web page, we need to identify which HTML tags contain the information we want. You can view the HTML source code of any web page in your browser by right-clicking on the page and selecting **\"View Page Source\"** (or pressing `Ctrl+U`).\n",
    "\n",
    "The HTML source of [httpbin.org/html](https://httpbin.org/html) looks like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "  </head>\n",
    "  <body>\n",
    "      <h1>Herman Melville - Moby Dick</h1>\n",
    "\n",
    "      <div>\n",
    "        <p>\n",
    "          Availing himself of the mild, summer-cool weather that now\n",
    "          reigned in these latitudes, and in preparation for the\n",
    "          peculiarly active pursuits shortly to be anticipated, Perth,\n",
    "          the begrimed, blistered old blacksmith, had not removed his\n",
    "          portable forge to the hold again ...\n",
    "        </p>\n",
    "      </div>\n",
    "  </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this HTML, we can see that:\n",
    "\n",
    "- The entire page is wrapped in `<html>` tags.\n",
    "- The visible content is inside the `<body>` tag.\n",
    "- The text we want is inside a `<p>` (paragraph) tag, which is nested inside a `<div>` tag.\n",
    "\n",
    "This tells us that to extract the text, we need to find the `<p>` tag and get its text content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requesting the web page\n",
    "\n",
    "Now we are ready to write some code. The first step is to **import** the Python modules (libraries) we need."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "print(\"Successfully imported necessary modules\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have imported three modules:\n",
    "\n",
    "- **`os`** — a built-in Python module for interacting with the operating system (e.g., creating folders).\n",
    "- **`requests`** — a popular module for making HTTP requests (i.e., downloading web pages).\n",
    "- **`BeautifulSoup`** (from the `bs4` package) — a module for parsing HTML and extracting information from it. We import it with the alias `soup` for convenience."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "link = \"https://httpbin.org/html\"\n",
    "response = requests.get(link)\n",
    "response.status_code"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down what just happened:\n",
    "\n",
    "1. We defined the URL of the page we want to scrape and stored it in a variable called `link`.\n",
    "2. We used `requests.get()` to send an HTTP GET request to that URL — this is the same thing your browser does when you visit a web page.\n",
    "3. The server's response is stored in a variable called `response`.\n",
    "4. We checked the **status code** of the response. A status code of **200** means the request was successful (the page was found and returned). Other common status codes include 404 (page not found) and 403 (access forbidden)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "response.text"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the web page\n",
    "\n",
    "The raw HTML is just a long string of text. To search through it and extract specific elements, we need to **parse** it — that is, turn it into a structured object that Python can navigate."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "soup_response = soup(response.text, \"html.parser\")\n",
    "soup_response"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the raw HTML text (`response.text`) to BeautifulSoup along with the parser we want to use (`\"html.parser\"`). The result, `soup_response`, is a BeautifulSoup object that we can search and navigate using Python methods. The output may look similar to the raw HTML, but it is now a structured object rather than a plain string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting information\n",
    "\n",
    "Now we can use BeautifulSoup's `.find()` method to locate the `<p>` tag and extract its text content."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "paragraph = soup_response.find(\"p\")\n",
    "paragraph"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data = paragraph.text\n",
    "print(data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results\n",
    "\n",
    "The final step is to save our extracted data to a file. First, we need to create a folder to store the output."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "try:\n",
    "    os.mkdir(\"./downloads\")\n",
    "except:\n",
    "    print(\"Folder already exists\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "outfile = \"./downloads/moby-dick-scraped-data.txt\"\n",
    "with open(outfile, \"w\") as f:\n",
    "    f.write(data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open(outfile, \"r\") as f:\n",
    "    print(f.read())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Page Scraping\n",
    "\n",
    "In practice, the data you need is rarely on a single page. A more realistic scenario involves collecting information spread across **multiple pages** of a website. In this section, we will scrape the **Edinburgh Council Warm Spaces Directory**, which lists organisations across the city that offer warm, welcoming spaces for members of the public.\n",
    "\n",
    "The directory is organised as an **A-to-Z listing**: there is a separate page for each letter of the alphabet (e.g., one page for organisations beginning with \"A\", another for \"B\", and so on). Each page contains a list of organisation names, each of which links to a detail page with further information (address, opening hours, etc.).\n",
    "\n",
    "This means we need **two loops**:\n",
    "1. A first loop to visit each A-Z page and collect the names and links of all organisations.\n",
    "2. A second loop to visit each organisation's detail page and extract the relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import string\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime as dt\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "data_folder = \"./data/\"\n",
    "try:\n",
    "    os.mkdir(data_folder)\n",
    "except:\n",
    "    print(\"Folder already exists\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the URL list\n",
    "\n",
    "To scrape all 26 pages, we need to construct the URL for each letter. The pattern is:\n",
    "\n",
    "> `https://www.edinburgh.gov.uk/directory/10258/a-to-z/A`\n",
    "> `https://www.edinburgh.gov.uk/directory/10258/a-to-z/B`\n",
    "> ... and so on.\n",
    "\n",
    "We can generate these URLs programmatically using Python's `string` module."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "header = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\"}\n",
    "base = \"https://www.edinburgh.gov.uk/directory/10258/a-to-z/\"\n",
    "abc = list(string.ascii_uppercase)\n",
    "print(abc)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note:\n",
    "\n",
    "- **`header`**: Some websites block requests that do not include a `User-Agent` header, because they look like automated bots rather than real browsers. By including a header that mimics a standard web browser, we make our requests look like normal web traffic. This is common practice in web scraping.\n",
    "- **`base`**: This is the base URL for the A-Z directory. We will append each letter of the alphabet to this base to construct the full URL for each page.\n",
    "- **`abc`**: `string.ascii_uppercase` gives us all 26 uppercase letters of the English alphabet as a list: `['A', 'B', 'C', ..., 'Z']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First loop: collect organisation names and links\n",
    "\n",
    "In this step, we loop over each letter of the alphabet, visit the corresponding A-Z page, and extract the name and link for every organisation listed on that page."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "org_list = []\n",
    "\n",
    "for letter in abc:\n",
    "    url = base + letter\n",
    "    response = requests.get(url, headers=header)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        page = soup(response.text, \"html.parser\")\n",
    "        try:\n",
    "            results = page.find(\"ul\", class_=\"list list--record\").find_all(\"li\")\n",
    "            for item in results:\n",
    "                name = item.find(\"a\").text\n",
    "                link = item.find(\"a\").get(\"href\")\n",
    "                org_list.append({\"org_name\": name, \"org_url\": link})\n",
    "        except:\n",
    "            print(f\"No organisations found for letter {letter}\")\n",
    "\n",
    "print(f\"Found {len(org_list)} organisations\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through the logic of this loop:\n",
    "\n",
    "1. We start with an empty list called `org_list` to store our results.\n",
    "2. For each letter in the alphabet, we construct the full URL by appending the letter to the base URL.\n",
    "3. We request the page and check that the status code is 200 (success).\n",
    "4. We parse the HTML and look for a `<ul>` tag with the class `\"list list--record\"` — this is the unordered list that contains the directory entries. Within that list, we find all `<li>` (list item) tags.\n",
    "5. For each list item, we extract the organisation name (the text inside the `<a>` tag) and the link (the `href` attribute of the `<a>` tag).\n",
    "6. We wrap this in a `try/except` block because some letters may have no organisations listed, which would cause an error.\n",
    "7. Each organisation is stored as a dictionary with two keys: `org_name` and `org_url`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "org_list[:5]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second loop: visit each organisation's page\n",
    "\n",
    "Now that we have a list of organisations and their URLs, we can visit each organisation's detail page and extract the information displayed there (e.g., address, opening hours, contact details)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "org_details = []\nbase_url = \"https://www.edinburgh.gov.uk\"\n\nfor org in org_list:\n    url = base_url + org[\"org_url\"]\n    response = requests.get(url, headers=header)\n    \n    if response.status_code == 200:\n        page = soup(response.text, \"html.parser\")\n        results = page.find(\"dl\", class_=\"list list--definition definition\")\n        \n        keys = [dt_tag.text.strip() for dt_tag in results.find_all(\"dt\")]\n        values = [dd.text.strip() for dd in results.find_all(\"dd\")]\n        \n        obs = dict(zip(keys, values))\n        obs[\"org_name\"] = org[\"org_name\"]\n        obs[\"org_url\"] = url\n        org_details.append(obs)\n    else:\n        print(f\"Could not request page for {org['org_name']}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "org_details[:3]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving to JSON\n",
    "\n",
    "We will save the collected data as a **JSON** file — a widely used, human-readable format for structured data. We include the current date in the filename so that we know when the data was collected."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ddate = dt.now().strftime(\"%Y-%m-%d\")\n",
    "outfile = f\"./data/edinburgh-warm-spaces-{ddate}.json\"\n",
    "\n",
    "with open(outfile, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(org_details, f)\n",
    "\n",
    "print(f\"Saved {len(org_details)} organisations to {outfile}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open(outfile, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "print(f\"Loaded {len(data)} records\")\n",
    "data[:2]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Now it's your turn! Using the same approach we used for the warm spaces directory, scrape the **Edinburgh Council Library Locations** directory:\n",
    "\n",
    "> [https://www.edinburgh.gov.uk/directory/10199/library-locations-and-opening-hours](https://www.edinburgh.gov.uk/directory/10199/library-locations-and-opening-hours)\n",
    "\n",
    "The URL pattern for the A-Z pages is:\n",
    "\n",
    "> `https://www.edinburgh.gov.uk/directory/10199/a-to-z/{letter}`\n",
    "\n",
    "Use the skeleton code below to guide you. Replace the `# INSERT CODE HERE` comments with your own code, following the same pattern as the warm spaces example above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Exercise: Scrape Edinburgh library locations\n",
    "# The URL pattern is: https://www.edinburgh.gov.uk/directory/10199/a-to-z/{letter}\n",
    "\n",
    "# Step 1: Define your variables\n",
    "header = {\"user-agent\": \"Mozilla/5.0\"}\n",
    "base = \"https://www.edinburgh.gov.uk/directory/10199/a-to-z/\"\n",
    "abc = list(string.ascii_uppercase)\n",
    "\n",
    "# Step 2: Loop over A-Z pages and collect library names and links\n",
    "library_list = []\n",
    "\n",
    "# INSERT CODE HERE\n",
    "\n",
    "print(f\"Found {len(library_list)} libraries\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 3: Visit each library page and extract details\n",
    "library_details = []\n",
    "\n",
    "# INSERT CODE HERE\n",
    "\n",
    "print(f\"Collected details for {len(library_details)} libraries\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 4: Save results to JSON\n",
    "\n",
    "# INSERT CODE HERE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, before scraping a website, always check whether the data is available through an **API** (Application Programming Interface). APIs provide structured, reliable access to data and avoid many of the legal and ethical issues associated with web scraping. We'll explore APIs in the next session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Exercise Solution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Exercise Solution: Scrape Edinburgh library locations\n\nimport string\nimport os\nimport requests\nimport json\nfrom datetime import datetime as dt\nfrom bs4 import BeautifulSoup as soup\n\n# Step 1: Define variables\nheader = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\"}\nbase = \"https://www.edinburgh.gov.uk/directory/10199/a-to-z/\"\nabc = list(string.ascii_uppercase)\nbase_url = \"https://www.edinburgh.gov.uk\"\n\n# Step 2: Loop over A-Z pages and collect library names and links\nlibrary_list = []\n\nfor letter in abc:\n    url = base + letter\n    response = requests.get(url, headers=header)\n    \n    if response.status_code == 200:\n        page = soup(response.text, \"html.parser\")\n        try:\n            results = page.find(\"ul\", class_=\"list list--record\").find_all(\"li\")\n            for item in results:\n                name = item.find(\"a\").text\n                link = item.find(\"a\").get(\"href\")\n                library_list.append({\"library_name\": name, \"library_url\": link})\n        except:\n            print(f\"No libraries found for letter {letter}\")\n\nprint(f\"Found {len(library_list)} libraries\")\n\n# Step 3: Visit each library page and extract details\nlibrary_details = []\n\nfor lib in library_list:\n    url = base_url + lib[\"library_url\"]\n    response = requests.get(url, headers=header)\n    \n    if response.status_code == 200:\n        page = soup(response.text, \"html.parser\")\n        results = page.find(\"dl\", class_=\"list list--definition definition\")\n        \n        keys = [dt_tag.text.strip() for dt_tag in results.find_all(\"dt\")]\n        values = [dd.text.strip() for dd in results.find_all(\"dd\")]\n        \n        obs = dict(zip(keys, values))\n        obs[\"library_name\"] = lib[\"library_name\"]\n        obs[\"library_url\"] = url\n        library_details.append(obs)\n    else:\n        print(f\"Could not request page for {lib['library_name']}\")\n\nprint(f\"Collected details for {len(library_details)} libraries\")\n\n# Step 4: Save results to JSON\nddate = dt.now().strftime(\"%Y-%m-%d\")\noutfile = f\"./data/edinburgh-libraries-{ddate}.json\"\n\nwith open(outfile, \"w\", encoding=\"utf-8\") as f:\n    json.dump(library_details, f)\n\nprint(f\"Saved {len(library_details)} libraries to {outfile}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**END OF FILE**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}