{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SGSSS Logo](../img/SGSSS_Stacked.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Digital Data for Social Scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Computational methods are transforming the social sciences, enabling researchers to collect, analyse, and interpret data at scales and speeds that were previously impossible. One of the most powerful techniques in this toolkit is **web scraping** — the automated extraction of information from websites. Web scraping allows social scientists to create new datasets from digital sources, turning the vast and often unstructured content of the internet into structured, analysable data.\n",
    "\n",
    "This practical session introduces you to web scraping using R. We will start with a simple example — extracting text from a single web page — and then move on to a more realistic scenario involving multiple pages. By the end of this session, you will have a solid foundation for collecting digital data from the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aims\n",
    "\n",
    "1. **Demonstrate how R can be used for web scraping** — from requesting web pages, to parsing HTML, extracting information, and saving results.\n",
    "2. **Cultivate computational thinking skills** — breaking down a data collection problem into a series of logical, repeatable steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Details\n",
    "\n",
    "| | |\n",
    "| --- | --- |\n",
    "| **Level** | Introductory |\n",
    "| **Time** | ~45 minutes |\n",
    "| **Pre-requisites** | None |\n",
    "| **Learning outcomes** | Understand the key steps involved in web scraping |\n",
    "| | Be able to use R to request a web page |\n",
    "| | Be able to use R to parse HTML content |\n",
    "| | Be able to use R to extract specific information from a web page |\n",
    "| | Be able to use R to save scraped data to a file |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide to Using This Resource\n",
    "\n",
    "This is a **Jupyter Notebook** — an interactive document that combines text, code, and output in a single environment. If you are viewing this in **Google Colab**, you are running the notebook in the cloud, which means you do not need to install anything on your own machine.\n",
    "\n",
    "**Note: This notebook uses R.** In Google Colab, you need to change the runtime: go to **Runtime > Change runtime type > select R**.\n",
    "\n",
    "A notebook is made up of **cells**. There are two main types:\n",
    "\n",
    "- **Markdown cells** contain formatted text (like this one). They provide explanations, instructions, and context.\n",
    "- **Code cells** contain R code that you can execute. Code cells are displayed with a grey background and have a play button on the left.\n",
    "\n",
    "To **run a cell**, click on it and press `Shift+Enter` (or click the play button). The output will appear directly below the cell. You should run the code cells **in order**, from top to bottom, as later cells often depend on variables or packages loaded in earlier cells.\n",
    "\n",
    "If you are new to Jupyter Notebooks and would like a more detailed introduction, see the excellent materials by Dani Arribas-Bel: [https://github.com/darribas/gds19/blob/master/content/labs/lab_00.ipynb](https://github.com/darribas/gds19/blob/master/content/labs/lab_00.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(c(\"httr\", \"rvest\", \"jsonlite\"))\n",
    "library(httr)\n",
    "library(rvest)\n",
    "library(jsonlite)\n",
    "cat(\"Successfully loaded packages\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name <- readline(\"Enter your name: \")\n",
    "cat(paste0(\"\\nHello \", name, \", enjoy learning about R and web scraping!\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Approach\n",
    "\n",
    "Web scraping follows a consistent pattern regardless of the website or the data you want to collect. Before writing any code, there are things you need to **KNOW** and things you need to **DO**.\n",
    "\n",
    "**What you need to KNOW:**\n",
    "- The **URL** (web address) of the page(s) containing the data you want.\n",
    "- The **HTML structure** of the page — specifically, which HTML tags and attributes contain the information you need.\n",
    "\n",
    "**What you need to DO:**\n",
    "1. **Request** the web page (download the HTML).\n",
    "2. **Parse** the HTML (turn the raw text into a structured, searchable object).\n",
    "3. **Extract** the specific information you need.\n",
    "4. **Save** the results to a file.\n",
    "\n",
    "This four-step pattern — request, parse, extract, save — is the foundation of nearly all web scraping tasks. It can be expressed as **pseudo-code**, which is an informal, plain-language description of the steps a program needs to follow. Writing pseudo-code before you write real code is an excellent habit: it helps you think through the logic of your task without getting bogged down in syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Text Extraction\n",
    "\n",
    "We will begin with a simple example: extracting a passage of text from a single web page. The website we will use is [httpbin.org/html](https://httpbin.org/html), which serves a short excerpt from *Moby Dick* by Herman Melville. This is a deliberately simple page, which makes it ideal for learning the basics of web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the web address\n",
    "\n",
    "The first thing we need to know is the **URL** of the page we want to scrape. In this case, the address is:\n",
    "\n",
    "> [https://httpbin.org/html](https://httpbin.org/html)\n",
    "\n",
    "If you visit this URL in your browser, you will see a short passage of text from *Moby Dick*. This is the data we want to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locating information in the HTML\n",
    "\n",
    "Web pages are written in **HTML** (HyperText Markup Language). HTML uses **tags** to structure content. For example, a paragraph of text is enclosed in `<p>` tags:\n",
    "\n",
    "```html\n",
    "<p>This is a paragraph.</p>\n",
    "```\n",
    "\n",
    "To scrape a web page, we need to identify which HTML tags contain the information we want. You can view the HTML source code of any web page in your browser by right-clicking on the page and selecting **\"View Page Source\"** (or pressing `Ctrl+U`).\n",
    "\n",
    "The HTML source of [httpbin.org/html](https://httpbin.org/html) looks like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "  </head>\n",
    "  <body>\n",
    "      <h1>Herman Melville - Moby Dick</h1>\n",
    "\n",
    "      <div>\n",
    "        <p>\n",
    "          Availing himself of the mild, summer-cool weather that now\n",
    "          reigned in these latitudes, and in preparation for the\n",
    "          peculiarly active pursuits shortly to be anticipated, Perth,\n",
    "          the begrimed, blistered old blacksmith, had not removed his\n",
    "          portable forge to the hold again ...\n",
    "        </p>\n",
    "      </div>\n",
    "  </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this HTML, we can see that:\n",
    "\n",
    "- The entire page is wrapped in `<html>` tags.\n",
    "- The visible content is inside the `<body>` tag.\n",
    "- The text we want is inside a `<p>` (paragraph) tag, which is nested inside a `<div>` tag.\n",
    "\n",
    "This tells us that to extract the text, we need to find the `<p>` tag and get its text content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requesting the web page\n",
    "\n",
    "Now we are ready to write some code. The first step is to **load** the R packages we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(httr)\n",
    "library(rvest)\n",
    "\n",
    "link <- \"https://httpbin.org/html\"\n",
    "response <- GET(link)\n",
    "status_code(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have loaded two packages:\n",
    "\n",
    "- **`httr`** — a package for making HTTP requests (i.e., downloading web pages). The `GET()` function sends a request to a URL, similar to what your browser does when you visit a page.\n",
    "- **`rvest`** — a package for parsing HTML and extracting information from it. It provides a set of functions for navigating and querying HTML documents.\n",
    "\n",
    "Let's break down what just happened:\n",
    "\n",
    "1. We defined the URL of the page we want to scrape and stored it in a variable called `link`.\n",
    "2. We used `GET()` to send an HTTP GET request to that URL.\n",
    "3. The server's response is stored in a variable called `response`.\n",
    "4. We checked the **status code** of the response. A status code of **200** means the request was successful (the page was found and returned). Other common status codes include 404 (page not found) and 403 (access forbidden)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the web page\n",
    "\n",
    "The raw HTML is just a long string of text. To search through it and extract specific elements, we need to **parse** it — that is, turn it into a structured object that R can navigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page <- read_html(content(response, \"text\"))\n",
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the raw HTML text (`content(response, \"text\")`) to `read_html()` from the `rvest` package. The result, `page`, is a parsed HTML document that we can search and navigate using R functions. The output may look different from the raw HTML, but it is now a structured object rather than a plain string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting information\n",
    "\n",
    "Now we can use `rvest`'s `html_element()` function to locate the `<p>` tag and `html_text2()` to extract its text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph <- html_text2(html_element(page, \"p\"))\n",
    "cat(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results\n",
    "\n",
    "The final step is to save our extracted data to a file. First, we need to create a folder to store the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir.create(\"downloads\", showWarnings = FALSE)\n",
    "writeLines(paragraph, \"downloads/moby-dick-scraped-data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readLines(\"downloads/moby-dick-scraped-data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Page Scraping\n",
    "\n",
    "In practice, the data you need is rarely on a single page. A more realistic scenario involves collecting information spread across **multiple pages** of a website. In this section, we will scrape the **Edinburgh Council Warm Spaces Directory**, which lists organisations across the city that offer warm, welcoming spaces for members of the public.\n",
    "\n",
    "The directory is organised as an **A-to-Z listing**: there is a separate page for each letter of the alphabet (e.g., one page for organisations beginning with \"A\", another for \"B\", and so on). Each page contains a list of organisation names, each of which links to a detail page with further information (address, opening hours, etc.).\n",
    "\n",
    "This means we need **two loops**:\n",
    "1. A first loop to visit each A-Z page and collect the names and links of all organisations.\n",
    "2. A second loop to visit each organisation's detail page and extract the relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(httr)\n",
    "library(rvest)\n",
    "library(jsonlite)\n",
    "\n",
    "dir.create(\"data\", showWarnings = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the URL list\n",
    "\n",
    "To scrape all 26 pages, we need to construct the URL for each letter. The pattern is:\n",
    "\n",
    "> `https://www.edinburgh.gov.uk/directory/10258/a-to-z/A`\n",
    "> `https://www.edinburgh.gov.uk/directory/10258/a-to-z/B`\n",
    "> ... and so on.\n",
    "\n",
    "We can generate these URLs programmatically using R's built-in `LETTERS` constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header <- add_headers(`User-Agent` = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\")\n",
    "base <- \"https://www.edinburgh.gov.uk/directory/10258/a-to-z/\"\n",
    "abc <- LETTERS\n",
    "print(abc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note:\n",
    "\n",
    "- **`header`**: Some websites block requests that do not include a `User-Agent` header, because they look like automated bots rather than real browsers. By including a header that mimics a standard web browser, we make our requests look like normal web traffic. This is common practice in web scraping. In R, we use `add_headers()` from the `httr` package to set custom headers.\n",
    "- **`base`**: This is the base URL for the A-Z directory. We will append each letter of the alphabet to this base to construct the full URL for each page.\n",
    "- **`abc`**: `LETTERS` is a built-in R constant that gives us all 26 uppercase letters of the English alphabet as a character vector: `\"A\" \"B\" \"C\" ... \"Z\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First loop: collect organisation names and links\n",
    "\n",
    "In this step, we loop over each letter of the alphabet, visit the corresponding A-Z page, and extract the name and link for every organisation listed on that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_list <- list()\n",
    "\n",
    "for (letter in abc) {\n",
    "  url <- paste0(base, letter)\n",
    "  response <- GET(url, header)\n",
    "\n",
    "  if (status_code(response) == 200) {\n",
    "    page <- read_html(content(response, \"text\"))\n",
    "    items <- tryCatch({\n",
    "      page |> html_elements(\"ul.list.list--record li\")\n",
    "    }, error = function(e) NULL)\n",
    "\n",
    "    if (!is.null(items) && length(items) > 0) {\n",
    "      for (item in items) {\n",
    "        name <- html_text2(html_element(item, \"a\"))\n",
    "        link <- html_attr(html_element(item, \"a\"), \"href\")\n",
    "        org_list <- append(org_list, list(list(org_name = name, org_url = link)))\n",
    "      }\n",
    "    } else {\n",
    "      cat(paste(\"No organisations found for letter\", letter, \"\\n\"))\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(paste(\"Found\", length(org_list), \"organisations\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through the logic of this loop:\n",
    "\n",
    "1. We start with an empty list called `org_list` to store our results.\n",
    "2. For each letter in the alphabet, we construct the full URL by pasting the letter onto the base URL using `paste0()`.\n",
    "3. We request the page using `GET()` and check that the status code is 200 (success).\n",
    "4. We parse the HTML using `read_html()` and look for `<li>` elements inside a `<ul>` tag with the class `\"list list--record\"` — this is the unordered list that contains the directory entries.\n",
    "5. For each list item, we extract the organisation name (the text inside the `<a>` tag using `html_text2()`) and the link (the `href` attribute of the `<a>` tag using `html_attr()`).\n",
    "6. We wrap the element selection in a `tryCatch()` block because some letters may have no organisations listed, which would cause an error.\n",
    "7. Each organisation is stored as a named list with two elements: `org_name` and `org_url`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_list[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second loop: visit each organisation's page\n",
    "\n",
    "Now that we have a list of organisations and their URLs, we can visit each organisation's detail page and extract the information displayed there (e.g., address, opening hours, contact details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_details <- list()\n",
    "base_url <- \"https://www.edinburgh.gov.uk\"\n",
    "\n",
    "for (org in org_list) {\n",
    "  url <- paste0(base_url, org$org_url)\n",
    "  response <- GET(url, header)\n",
    "\n",
    "  if (status_code(response) == 200) {\n",
    "    page <- read_html(content(response, \"text\"))\n",
    "    dl <- html_element(page, \"dl.list.list--definition.definition\")\n",
    "\n",
    "    keys <- html_text2(html_elements(dl, \"dt\"))\n",
    "    values <- html_text2(html_elements(dl, \"dd\"))\n",
    "\n",
    "    obs <- setNames(as.list(values), keys)\n",
    "    obs$org_name <- org$org_name\n",
    "    obs$org_url <- url\n",
    "    org_details <- append(org_details, list(obs))\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(paste(\"Collected details for\", length(org_details), \"organisations\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving to JSON\n",
    "\n",
    "We will save the collected data as a **JSON** file — a widely used, human-readable format for structured data. We include the current date in the filename so that we know when the data was collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(org_details, paste0(\"data/edinburgh-warm-spaces-\", Sys.Date(), \".json\"))\n",
    "cat(\"Saved!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Now it's your turn! Using the same approach we used for the warm spaces directory, scrape the **Edinburgh Council Library Locations** directory:\n",
    "\n",
    "> [https://www.edinburgh.gov.uk/directory/10199/library-locations-and-opening-hours](https://www.edinburgh.gov.uk/directory/10199/library-locations-and-opening-hours)\n",
    "\n",
    "The URL pattern for the A-Z pages is:\n",
    "\n",
    "> `https://www.edinburgh.gov.uk/directory/10199/a-to-z/{letter}`\n",
    "\n",
    "Use the skeleton code below to guide you. Replace the `# INSERT CODE HERE` comments with your own code, following the same pattern as the warm spaces example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Scrape Edinburgh library locations\n",
    "# The URL pattern is: https://www.edinburgh.gov.uk/directory/10199/a-to-z/{letter}\n",
    "\n",
    "# Step 1: Define your variables\n",
    "header <- add_headers(`User-Agent` = \"Mozilla/5.0\")\n",
    "base <- \"https://www.edinburgh.gov.uk/directory/10199/a-to-z/\"\n",
    "abc <- LETTERS\n",
    "base_url <- \"https://www.edinburgh.gov.uk\"\n",
    "\n",
    "# Step 2: Loop over A-Z pages and collect library names and links\n",
    "library_list <- list()\n",
    "\n",
    "# INSERT CODE HERE\n",
    "\n",
    "cat(paste(\"Found\", length(library_list), \"libraries\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Visit each library page and extract details\n",
    "library_details <- list()\n",
    "\n",
    "# INSERT CODE HERE\n",
    "\n",
    "cat(paste(\"Collected details for\", length(library_details), \"libraries\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Save results to JSON\n",
    "\n",
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, before scraping a website, always check whether the data is available through an **API** (Application Programming Interface). APIs provide structured, reliable access to data and avoid many of the legal and ethical issues associated with web scraping. We'll explore APIs in the next session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Exercise Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise Solution: Scrape Edinburgh library locations\n",
    "\n",
    "library(httr)\n",
    "library(rvest)\n",
    "library(jsonlite)\n",
    "\n",
    "# Step 1: Define variables\n",
    "header <- add_headers(`User-Agent` = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\")\n",
    "base <- \"https://www.edinburgh.gov.uk/directory/10199/a-to-z/\"\n",
    "abc <- LETTERS\n",
    "base_url <- \"https://www.edinburgh.gov.uk\"\n",
    "\n",
    "# Step 2: Loop over A-Z pages and collect library names and links\n",
    "library_list <- list()\n",
    "\n",
    "for (letter in abc) {\n",
    "  url <- paste0(base, letter)\n",
    "  response <- GET(url, header)\n",
    "\n",
    "  if (status_code(response) == 200) {\n",
    "    page <- read_html(content(response, \"text\"))\n",
    "    items <- tryCatch({\n",
    "      page |> html_elements(\"ul.list.list--record li\")\n",
    "    }, error = function(e) NULL)\n",
    "\n",
    "    if (!is.null(items) && length(items) > 0) {\n",
    "      for (item in items) {\n",
    "        name <- html_text2(html_element(item, \"a\"))\n",
    "        link <- html_attr(html_element(item, \"a\"), \"href\")\n",
    "        library_list <- append(library_list, list(list(library_name = name, library_url = link)))\n",
    "      }\n",
    "    } else {\n",
    "      cat(paste(\"No libraries found for letter\", letter, \"\\n\"))\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(paste(\"Found\", length(library_list), \"libraries\\n\"))\n",
    "\n",
    "# Step 3: Visit each library page and extract details\n",
    "library_details <- list()\n",
    "\n",
    "for (lib in library_list) {\n",
    "  url <- paste0(base_url, lib$library_url)\n",
    "  response <- GET(url, header)\n",
    "\n",
    "  if (status_code(response) == 200) {\n",
    "    page <- read_html(content(response, \"text\"))\n",
    "    dl <- html_element(page, \"dl.list.list--definition.definition\")\n",
    "\n",
    "    keys <- html_text2(html_elements(dl, \"dt\"))\n",
    "    values <- html_text2(html_elements(dl, \"dd\"))\n",
    "\n",
    "    obs <- setNames(as.list(values), keys)\n",
    "    obs$library_name <- lib$library_name\n",
    "    obs$library_url <- url\n",
    "    library_details <- append(library_details, list(obs))\n",
    "  } else {\n",
    "    cat(paste(\"Could not request page for\", lib$library_name, \"\\n\"))\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(paste(\"Collected details for\", length(library_details), \"libraries\\n\"))\n",
    "\n",
    "# Step 4: Save results to JSON\n",
    "outfile <- paste0(\"data/edinburgh-libraries-\", Sys.Date(), \".json\")\n",
    "write_json(library_details, outfile)\n",
    "cat(paste(\"Saved\", length(library_details), \"libraries to\", outfile, \"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**END OF FILE**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "R"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}